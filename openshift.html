<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>OpenShift for application developers</title>
<meta name="description" content="OpenShift 4 quick reference for application developers using CodeReady containers.">
<meta name="viewport" content="width=devide-width, initial-scale=1">
<link rel="apple-touch-icon" href="icon.png">
<link rel="stylesheet" href="unstyle.css">
<link rel="stylesheet" href="style.css">
<link rel="stylesheet" href="print.css" media="print">
</head>
<body>

<div id="pages" class="clearfix">

<div>

  <h1 id="title"><a href="#title"></a>WIP! OpenShift for application developers</h1>

  <p>This text is a quick reference to OpenShift 4 with examples for developers. It shows steps for setting up a test cluster using CodeReady containers (CRC 1.37.0, OpenShift client and server 4.9.10, Kubernetes v1.22.3) on Fedora 34.</p>

  <p>In the first section the CRC cluster is set up with users, projects, and privileges. This setup is used in the later sections containing examples of basic usage.</p>

  <h2 id="1-1"><a href="#1-1"></a>Set up CodeReady containers</h2>

  <p>Download <code>crc</code> and your pull secret from <a href="https://console.redhat.com/openshift/create/local">console.redhat.com/openshift/create/local</a> (with a RedHat account). Then create a local OpenShift 4 cluster as shown below (or follow RedHat's CRC getting started guide). Virtualization must be enabled in the motherboard's boot menu.</p>

  <div class="code"><pre><code
>tar xvf crc-linux-amd64.tar.xz   <span class="comment"># extract downloaded crc binary</span>
mv crc-linux-*-amd64/crc ~/bin/  <span class="comment"># move crc to a $PATH location</span>

crc version               <span class="comment"># show crc and oc version</span>
crc setup                 <span class="comment"># set up cluster</span>
crc start -p pull-secret  <span class="comment"># start cluster with pull secret</span>
eval $(crc oc-env)        <span class="comment"># add oc to $PATH</span>
. &lt;(oc completion bash)   <span class="comment"># add bash completion (requires bash-completion package)</span>
EDITOR=vim                <span class="comment"># use vim for oc edit</span>

ifconfig                  <span class="comment"># view virtual network interfaces</span>
cat /etc/hosts            <span class="comment"># view host names added by crc</span>

<span class="comment"># login to cluster as kubeadmin or as developer user</span>
crc console --credentials                                  <span class="comment"># show usernames and passwords</span>
oc login -u developer https://api.crc.testing:6443         <span class="comment"># login as developer (no password)</span>
oc login -u kubeadmin -p xxx https://api.crc.testing:6443  <span class="comment"># login as kubeadmin</span>
crc console     <span class="comment"># open web console in browser (login as developer with password)</span>
oc config view  <span class="comment"># show current login context configuration</span>

crc stop                  <span class="comment"># stop the cluster</span>
crc start -p pull-secret  <span class="comment"># start the cluster</span></code></pre>
  </div>

  <p>A warning will show up on cluster start if a CRC update is available. Update <code>crc</code> by downloading the new binary, stopping and deleting the cluster, overwriting the old <code>crc</code> binary, running setup, and finally running start with the same pull secret.</p>

  <div class="code"><pre><code
>crc stop
crc delete   <span class="comment"># delete the cluster</span>
tar xvf crc-linux-amd64.tar.xz
mv crc-linux-*-amd64/crc ~/bin/
crc version
crc setup
crc start -p pull-secret</code></pre>
  </div>

  <h3 id="1-1-1"><a href="#1-1-1"></a>Create users, projects, and service accounts</h3>

  <p>This section shows how to create user accounts, projects, service accounts, and associated RBAC policies. Later sections assume these exist.</p>

  <h4 id="1-1-1-1"><a href="#1-1-1-1"></a>Create users</h4>

  <p>The CRC OpenShift cluster uses HTPasswd as the identity provider for users. This is configured in the custom resource <code>OAuth</code> named <code>cluster</code>.</p>

    <p>User passwords are generated by <code>htpasswd</code> and are added to the Kubernetes secret <code>htpass-secret</code> by the cluster administrator in the namespace <code>openshift-config</code>. When users log in, their passwords are checked against the stored secret and if authenticated they are issued an OAuth token.</p>

</div>

<div>

  <p>Extract the password file from the secret, then use <code>htpasswd</code> to add new users and passwords to it, and finally add it to the HTPasswd secret.</p>

  <div class="code"><pre><code
>sudo dnf install httpd-tools  <span class="comment"># install htpasswd</span>
oc login -u kubeadmin -p xxx https://api.crc.testing:6443

oc explain oauth.spec.identityProviders.htpasswd.fileData  <span class="comment"># view OAuth documentation</span>
<span class="comment"># check that identity provider is htpasswd and that fileData.name is "htpass-secret"</span>
oc get oauth cluster -o yaml | less

<span class="comment"># get password file and update it</span>
oc extract secret/htpass-secret -n openshift-config --keys=htpasswd
mv htpasswd htpasswd.tmp
sed -i '$a\' htpasswd.tmp       <span class="comment"># append newline if missing</span>
htpasswd -B htpasswd.tmp admin  <span class="comment"># update the htpasswd password file; prompts for password</span>
htpasswd -B htpasswd.tmp linda
htpasswd -B htpasswd.tmp other

<span class="comment"># update the secret's data field and wait until oauth pod restarts</span>
oc set data secret/htpass-secret -n openshift-config --from-file=htpasswd=htpasswd.tmp
oc get pods -n openshift-authentication --watch

<span class="comment"># log in (which creates the User and Identity resources)</span>
oc login -u admin https://api.crc.testing:6443
oc login -u linda https://api.crc.testing:6443
oc login -u other https://api.crc.testing:6443</code></pre>
  </div>

  <h4 id="1-1-1-2"><a href="#1-1-1-2"></a>Create projects</h4>

  <p>Create new projects as cluster administrator. How to group resources into projects depends on how it simplifies maintenance of things like access privileges, shared resources, network flows, and pods' node placement.</p>

  <div class="code"><pre><code
  >oc adm new-project env1 --admin=linda
oc adm new-project env2 --admin=other
oc adm new-project qa  <span class="comment"># for testing tools</span>
oc adm new-project db  <span class="comment"># for database resources</span>
oc adm new-project common  <span class="comment"># for common resources</span></code></pre>
  </div>

  <p>Default resources <code>ResourceQuota</code>, <code>LimitRange</code>, <code>NetworkPolicy</code>, and <code>ServiceAccount</code> are added to each project. Each resource YAML is added to <code>project-resources.yaml</code>.</p>

  <p>A project quota YAML is created below. This sets the maximum CPU and memory that a project's containers can request and use in total.</p>
  
  <div class="code"><pre><code
>oc create quota memory-and-cpu-quota --dry-run=client -o yaml \
  --hard=limits.memory=2Gi,limits.cpu=2,requests.memory=2Gi,requests.cpu=2 \
  &gt;&gt; project-resources.yaml</code></pre>
  </div>
    
  <p>Below is project limit range that limits container's CPU and memory request and limits. This sets the defaults and allowed range.</p>

  <div class="code"><pre><code
  >cat &gt;&gt; project-resources.yaml &lt;&lt;&lt;'
---
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-and-cpu-default-and-range
spec:
  limits:
  - type: Container
    default:
      memory: 512Mi
      cpu: 500m
    defaultRequest:
      memory: 512Mi
      cpu: 500m
    max:
      memory: 2Gi
      cpu: 2
    min:
      memory: 32Mi
      cpu: 100m'</code></pre>
  </div>

  <p>The following network policy allows only traffic incoming from exposed routes and outgoing DNS queries. Egress policies are silently ignored in CRC software network though.</p>

</div>

<div>

  <div class="code"><pre><code
>cat &gt;&gt; project-resources.yaml &lt;&lt;&lt;'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-route-ingress-and-dns-egress
spec:
  podSelector: {}
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""
  egress:
  - to:
    ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP
  policyTypes:
  - Egress
  - Ingress'</code></pre>
  </div>

  <p>The following network policy allows traffic between pods running within the same project (i.e. same namespace).</p>

  <div class="code"><pre><code
>cat &gt;&gt; project-resources.yaml &lt;&lt;&lt;'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: same-namespace
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}
  egress:
  - to:
    - podSelector: {}
  policyTypes:
  - Ingress
  - Egress'</code></pre>
  </div>

  <p>A service account YAML is created below. It will manually get role bindings in later sections. A project always gets service accounts <code>builder</code>, <code>deployer</code>, and <code>default</code> with role bindings to run builds, deployments, and application pods.</p>

  <div class="code"><pre><code
>echo $'\n---' &gt;&gt; project-resources.yaml
oc create serviceaccount system1 --dry-run=client -o yaml &gt;&gt; project-resources.yaml</code></pre>
  </div>

  <p>Create the resources in each project.</p>

  <div class="code"><pre><code
>oc create -f project-resources.yaml -n env1
oc create -f project-resources.yaml -n env2
oc create -f project-resources.yaml -n qa
oc create -f project-resources.yaml -n db
oc create -f project-resources.yaml -n common</code></pre>
      </div>

  <p>Logged in users can self-provision a project with <code>oc new-project</code>. A template can then also be configured with the additional resources to be added automatically. This isn't used for this text, but can be configured as shown below.</p>
  
  <div class="code"><pre><code
>oc adm create-bootstrap-project-template -o yaml > project-template.yaml
<span class="comment"># ... add resources to project-template.yaml objects-list...</span>

oc create -f project-template.yaml -n openshift-config <span class="comment"># upload template</span>
oc get template project-request -n openshift-config -o yaml  <span class="comment"># view the template</span>

<span class="comment"># edit configuration to add template name to spec as:
# spec:
#   projectRequestTemplate:
#     name: project-request</span>
oc edit project.config.openshift.io/cluster
oc get pods -n openshift-apiserver --watch  <span class="comment"># wait for api-server restart</span>
oc new-project PROJECTNAME  <span class="comment"># request project</span></code></pre>
  </div>

</div>

<div>

  <h4 id="1-1-1-3"><a href="#1-1-1-3"></a>Create RBAC rules</h4>

  <p>Role-based access control (RBAC) grants or denies requests to perform an action on a resource. A role is created with the verbs and resources that it allows. The role is bound to either a user, a service account, or a group. <code>ClusterRole</code> and <code>ClusterRoleBinding</code> are not project-namespaced while <code>Role</code> and <code>RoleBinding</code> are.</p>

  <p>Service accounts run pods to provide them with RBAC privileges, security contexts, and access to secrets. <code>default</code> runs pods when no service account is specified. <code>builder</code> ruins build pods. <code>deployer</code> runs deploy pods.</p>

  <p>The user <code>admin</code> is made cluster administrator by adding it to a group that is bound to the role <code>cluster-admin</code>.</p>

  <div class="code"><pre><code
>oc login -u kubeadmin -p xxx https://api.crc.testing:6443  <span class="comment"># log in as cluster admin</span>
oc describe clusterrole.rbac         <span class="comment"># show all cluster roles</span>
oc describe clusterrolebinding.rbac  <span class="comment"># show all cluster role bindings</span>

<span class="comment"># create a new group, bind role to it, and add admin</span>
oc adm groups new cluster-administrators
oc adm policy add-cluster-role-to-group cluster-admin cluster-administrators \
  --rolebinding-name=cluster-administrators
oc adm groups add-users cluster-administrators admin

oc auth can-i '*' '*' --all-namespaces --as=admin --as-group=cluster-administrators
oc login -u admin https://api.crc.testing:6443  <span class="comment"># continue as new cluster administator</span></code></pre>
  </div>

  <p>Add edit-privileges to users in the projects <code>qa</code>, <code>db</code>, and <code>common</code>.</p>

  <div class="code"><pre><code
>oc adm policy add-role-to-user edit linda other -n qa     --rolebinding-name=users-edit
oc adm policy add-role-to-user edit linda other -n db     --rolebinding-name=users-edit
oc adm policy add-role-to-user edit linda other -n common --rolebinding-name=users-edit</code></pre>
</div>

  <p>Add the security context constraint (SCC) <code>anyuid</code> to the service account <code>system1</code> in its project. Service accounts automatically have the role <code>system:image-puller</code> to run pods in the project. The <code>anyuid</code> SCC lets pods run with any user ID (including root).</p>

  <div class="code"><pre><code
>oc adm policy add-scc-to-user anyuid --serviceaccount=system1 -n env1
oc adm policy add-scc-to-user anyuid --serviceaccount=system1 -n env2</code></pre>
  </div>

  <p>Add <code>system:image-pusher</code> in project <code>common</code> to each <code>builder</code> in <code>env1</code> and <code>env2</code> to allow builds to push images to <code>common</code>. Add <code>system:image-puller</code> to all service accounts in the other projects (ignore the warning that the group doesn't exist).</p>

  <div class="code"><pre><code
>oc adm policy add-role-to-user system:image-pusher -n common \
  system:serviceaccount:env1:builder \
  system:serviceaccount:env2:builder \
  --rolebinding-name=builder-push

oc adm policy add-role-to-group system:image-puller -n common \
  system:serviceaccounts:env1 system:serviceaccounts:env2 \
  system:serviceaccounts:qa system:serviceaccounts:db \
  --rolebinding-name=serviceaccounts-pull</code></pre>
  </div>

  <p>Service accounts automatically get their own docker-secret linked for authentication to the internal registry (i.e. a <code>.dockercfg</code> file).</p>

  <h4 id="1-1-1-4"><a href="#1-1-1-4"></a>Create network policies</h3>

  <p>A network policy adds rules on allowed traffic to pods. The policy selects which pods to apply to using labels for either pods or namespaces (projects). Allowing only a single direction still allows duplex communication, but only the egress-to-ingress-direction may initiate the communication. All egress policies are silently ignored in the network controller used in the CRC cluster, but are created here anyway.</p>

</div>

<div>

  <p>If a project should allow all traffic, then it's more efficient to delete all its network policies. The default protocol in a network policy is TCP; allowed values are: TCP, UDP, and SCTP.</p>

  <p>Two network policies are parameterized in a template below. They allow <code>egress</code>-labeled pods to communicate with <code>ingress</code>-labeled pods from the selected namespace.</p>

  <div class="code"><pre><code
>cat &gt; template.networkpolicy.yaml &lt;&lt;&lt;'
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: template-networkpolicy
parameters:
- name: NAMESPACE
  required: true
  description: Which network to allow traffic between egress-labeled and ingress-labeled.
objects:
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allowed-ingress-${NAMESPACE}
  spec:
    podSelector:
      matchLabels:
        ingress: "true"
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network: ${NAMESPACE}
    policyTypes:
    - Ingress
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allowed-egress-${NAMESPACE}
  spec:
    podSelector:
      matchLabels:
        egress: "true"
    egress:
    - to:
      - namespaceSelector:
          matchLabels:
            network: ${NAMESPACE}
    policyTypes:
    - Egress'</code></pre>
    </div>

  <p>The projects must be labeled to be matched by a network policy. The cluster administrator adds label <code>namespace</code> on the projects as below, on the namespace resource.</p>

  <div class="code"><pre><code
>oc label namespace env1    namespace=env1
oc label namespace env2    namespace=env2
oc label namespace qa      namespace=qa
oc label namespace db      namespace=db
oc label namespace common  namespace=common
oc get namespace --show-labels -l=namespace</code></pre>
  </div>

  <p>Create the templated network policies in projects to open traffic between their <code>ingress</code>- and <code>egress</code>-labeled pods.</p>

  <div class="code"><pre><code
>oc process -f template.networkpolicy.yaml -p NAMESPACE=env2 | oc create -f - -n env1
oc process -f template.networkpolicy.yaml -p NAMESPACE=env1 | oc create -f - -n env2
oc process -f template.networkpolicy.yaml -p NAMESPACE=db   | oc create -f - -n env1
oc process -f template.networkpolicy.yaml -p NAMESPACE=db   | oc create -f - -n env2
oc process -f template.networkpolicy.yaml -p NAMESPACE=env1 | oc create -f - -n db
oc process -f template.networkpolicy.yaml -p NAMESPACE=env2 | oc create -f - -n db</code></pre>
  </div>

  <h3 id="1-1-2"><a href="#1-1-2"></a>Access internal image registry with Podman and Skopeo</h3>
  
  <p>The OpenShift internal registry should already be configured with the registry service <code>image-registry.openshift-image-registry.svc:5000</code> and its external facing route <code>default-route-openshift-image-registry.apps-crc.testing</code>.</p>

</div>

<div>


  <div class="code"><pre><code
><span class="comment"># confirm that service and route exist for the internal registry</span>
oc get service,route -n openshift-image-registry
oc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}'
oc describe service image-registry -n openshift-image-registry

oc registry info --public    <span class="comment"># this should show the external URL to the internal registry</span>
oc registry info --internal  <span class="comment"># this should show the internal URL to the internal registry</span>

<span class="comment"># when spec.defaultRoute is true in this custom resource the default registry route is created</span>
oc get configs.imageregistry.operator.openshift.io cluster -o yaml | grep 'defaultRoute'</code></pre>
  </div>

  <p>To access the route from outside the cluster, extract the <code>ingress-operator</code>'s CA certificate from its <code>ConfigMap</code> and then use it and a user token to log in with <code>podman</code>, as shown below. </p>

  <p>RBAC rules to access images and imagestreams et cetera are needed to pull and/or push images within a project. The roles <code>registry-viewer</code> and <code>registry-editor</code> have the necessary access privileges. Project roles <code>view</code> and <code>edit</code> include those privileges.</p>

  <div class="code"><pre><code
>oc login -u admin  <span class="comment"># use cluster admin for unrestricted access to search the registry</span>

<span class="comment"># use route URL from outside the cluster</span>
ROUTE=$(oc registry info --public)

<span class="comment"># get the ingress-operator CA certificate as a file (named ${ROUTE})</span>
oc extract configmap/registry-certs -n openshift-config --keys=${ROUTE}
<span class="comment"># inspect same certificate file from route (CN=ingress-operator@XXX)</span>
openssl x509 -text -noout -in ${ROUTE}
<span class="comment"># view certificate chain (ingress-operator signs *.app-crc.testing)</span>
openssl s_client -showcerts -CAfile ${ROUTE} \
  -connect ${ROUTE}:443 \
  &lt; /dev/null

<span class="comment"># move certificate for usage with podman/skopeo/buildah</span>
mkdir -p "${HOME}/.config/containers/certs.d/${ROUTE}"
mv ${ROUTE} "${HOME}/.config/containers/certs.d/${ROUTE}/ca.crt"

<span class="comment"># log in with podman using admin's token</span>
podman login --username $(oc whoami) --password "$(oc whoami --show-token)" ${ROUTE}

<span class="comment"># search the internal registry</span>
podman search "${ROUTE}/"
podman search --list-tags "${ROUTE}/openshift/httpd"
skopeo inspect "docker://${ROUTE}/openshift/httpd:latest"

<span class="comment"># push image to internal registry in project common</span>
podman push myimage:123 "docker://${ROUTE}/common/myimage:123"
skopeo copy containers-storage:myimage:456 "docker://${ROUTE}/common/myimage:456"
oc describe imagestream myimage -n common     <span class="comment"># imagestream was created in common</span>
oc describe imagestreamtag myimage:456 -n common  <span class="comment"># imagestreamtag was created in common</span>

<span class="comment"># pull image to host</span>
podman pull "docker://${ROUTE}/common/myimage:123"
skopeo copy "docker://${ROUTE}/common/myimage:123" containers-storage:localhost/myimage:123</code></pre>
  </div>

  <p>Deleting the image stream doesn't remove image data. Images can be pruned manually, as shown below. <code>skopeo delete</code> is not allowed. The node performs garbage collection every few minutes.</p>

  <div class="code"><pre><code
>oc adm prune images  <span class="comment"># dry-run image prune</span>
<span class="comment"># prune images (from outside cluster using the route)</span>
oc adm prune images --registry-url=${ROUTE} --confirm

oc adm top images  <span class="comment"># view all managed images with usage statistics</span>

<span class="comment"># check disk/inode usage on node (causing node.kubernetes.io/disk-pressure)</span>
oc debug node/$(oc get node -o jsonpath='{.items[0].metadata.name}') -- df -h
oc debug node/$(oc get node -o jsonpath='{.items[0].metadata.name}') -- df -i</code></pre>
  </div>

</div>

<div>

  <h2 id="1-2"><a href="#1-2"></a>Starting images</h2>

  <p>This sections shows various commands for running containers as applications in OpenShift.</p>

  <p>OpenShift resources are declared in either JSON or YAML, but the command-line tools assist in resource creation and management. Most commands have a dry-run option which outputs a resource declaration file that can be used with <code>oc create -f FILE</code>.</p>

  <p>The command <code>oc run</code> runs an image as a stand-alone pod.</p>

  <div class="code"><pre><code
>oc login -u linda && oc project env1

<span class="comment"># run a container in a pod</span>
oc run mysqlpod --image=registry.redhat.io/rhel8/mysql-80:1-159 --restart=Never --port=3306 \
  --env=MYSQL_USER=user --env=MYSQL_PASSWORD=pass --env=MYSQL_DATABASE=dbname
oc cp quote.sql mysqlpod:/tmp/quote.sql
oc rsh -t mysqlpod mysql -P3306 -uuser -ppass dbname &lt; /tmp/quote.sql  <span class="comment"># run mysql in a shell</span>

<span class="comment"># run a command (unqualified image is searched for in configured registries)</span>
oc run ubipod --image=ubi8/ubi:latest --restart=Never --rm -it --command -- bash -il

<span class="comment"># run an image from the internal registry</span>
oc run PODNAME --image=$(oc registry info --public)/PROJECT/IMAGE:TAG \
  --image-pull-policy="Always"  <span class="comment"># makes node agent pull image before running</span>

<span class="comment"># run as service account with SCC anyuid to run as root</span>
oc run nginxpod --image=docker.io/library/nginx:1.21.5-alpine --restart=Never \
  --overrides='{"spec":{"serviceAccountName":"system1"}}'
oc exec nginxpod -- nginx -T  <span class="comment"># view configuration files</span>
oc create service clusterip nginxpod --tcp=8080:80  <span class="comment"># create DNS entry to pod</span>
oc label pod nginxpod app=nginxpod  <span class="comment"># add label to match service's selector</span>
oc expose service nginxpod  <span class="comment"># create route for cluster-external clients</span>
curl "http://$(oc get route nginxpod --template='{{.spec.host}}'):80"

<span class="comment"># run oc client image (with writable $HOME/.kube)</span>
oc run oc --image=registry.redhat.io/openshift4/ose-cli:v4.9 \
  --restart=Never --rm -it --env=HOME=/tmp</code></pre>
  </div>

  <p>Instead of creating stand-alone pods with <code>oc run</code>, the command <code>oc new-app</code> when given an image creates a deployment that manages the pod.</p>

  <div class="code"><pre><code
>oc new-app --name=mysqlapp --image=registry.redhat.io/rhel8/mysql-80:1-159 \
  -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=dbname
oc delete all -l app=mysqlapp  <span class="comment"># delete the created resources with the label app=NAME</span></code></pre>
  </div>

  <p>OpenShift runs containers with a random user ID and the root group. So by default containers can't use privileged ports and should be granted file access through its root-group. A cluster administrator can however run privileged containers as root without configuration.</p>

  <p>View a node's registry configuration files with a debug shell into it. Some image names have aliases and unqualified names are searched in registry.access.redhat.com and docker.io.</p>

  <div class="code"><pre><code
><span class="comment"># run shell in the node and view registry configuration</span>
oc debug node/$(oc get node -o jsonpath='{.items[0].metadata.name}')
cat /host/etc/containers/{registries.conf,registries.conf.d/*}</code></pre>
  </div>

  <p>When a kubelet ("node agent") pulls images for pods it uses the secret <code>pull-secret</code> in the namespace <code>openshift-config</code>, or the <code>docker-registry</code> secret in a pod's <code>imagePullSecrets</code> list, or the <code>docker-registry</code> linked to the service account running the pod. Registries not configured in pull secrets are accessed anonymously.</p>

  <div class="code"><pre><code
><span class="comment"># get the global pull-secret, add credentials to it, and update the secret</span>
oc extract secret/pull-secret -n openshift-config --keys=.dockerconfigjson
oc registry login --registry=docker.io --auth-basic=USERNAME:PASSWORD --to=.dockerconfigjson
oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson
oc get pods --all-namespaces -w  <span class="comment"># wait until affected pods are restarted</span></code></pre>
  </div>

</div>

<div>

  <p>Below, a pull secret is created in the project <code>common</code> instead of globally. It is then linked to service accounts in the project.</p>

  <div class="code"><pre><code
><span class="comment"># create a registry authentication secret in common project</span>
oc project common
oc create secret docker-registry mydockerhubsecret \
  --docker-server=docker.io \
  --docker-username=USERNAME --docker-password=PASSWORD
oc extract secret/dockerhub --to=-  <span class="comment"># view the secret data</span>

<span class="comment"># link for 'pull' for kubelet to pull and run</span>
oc secrets link default mydockerhubsecret --for=pull
<span class="comment"># link for 'mount' for build pod container to pull and push</span>
oc secrets link builder mydockerhubsecret --for=pull,mount</code></pre>
  </div>

  <p>Podman can also be used to generate the secret's configuration file. Create the secret as <code>generic</code> and with <code>--type=kubernetes.io/dockerconfigjson</code> and the file as data.</p>

  <div class="code"><pre><code
><span class="comment"># generate configuration file named AUTHFILE</span>
podman login docker.io --authfile=AUTHFILE  <span class="comment"># prompts for username and password</span>
oc create secret generic mydockerhubsecret \
  --from-file=.dockerconfigjson=AUTHFILE --type=kubernetes.io/dockerconfigjson</code></pre>
  </div>

  <p>Note that for a system account to access images from another projects' image streams, it needs the RBAC role <code>system:image-puller</code> and <code>system:image-pusher</code> in that other project.</p>

  <p>A pod container's <code>imagePullPolicy</code> determines if a node pulls the images before running it. Static versioned tags can have <code>"IfNotPresent"</code> since their image data won't change. If <code>imagePullPolicy</code> is not specified, images tagged <code>"latest"</code> result in the image pull policy being <code>"Always"</code> (since it's a floating tag). Other tags default to <code>"IfNotPresent"</code>.</p>

  <h3 id="1-2-1"><a href="#1-2-1"></a>Using OpenShift image streams</h3>

  <p>An OpenShift image stream tracks image updates by storing the immutable image digests. Each image stream tag has a history of image digests and is updated with the latest digest when importing the tracked image. Updating an image stream tag also be set to trigger deployments with the new image digest.</p>
  
  <p>Use <code>oc import-image</code> to update an image stream tag's most recent image digest. Use <code>oc tag</code> to manipulate image stream tags.</p>

  <div class="code"><pre><code
><span class="comment"># create imagestream and imagestreamtag</span>
oc tag REGISTRY/IMAGE:TAG IMAGESTREAM:IMAGESTREAMTAG
<span class="comment"># create imagestream and imagestreamtag</span>
oc import-image IMAGESTREAM --from=REGISTRY/IMAGE:TAG --confirm

<span class="comment"># update imagestreamtag with newest image</span>
oc import-image IMAGESTREAM:IMAGESTREAMTAG

<span class="comment"># create imagestream and imagestreamtag with internal registry as pullthrough mirror</span>
oc tag --reference-policy=local REGISTRY/IMAGE:TAG IMAGESTREAM:IMAGESTREAMTAG

<span class="comment"># create imagestream and imagestreamtag with scheduled import from external registry</span>
oc tag --scheduled REGISTRY/IMAGE:TAG IMAGESTREAM:IMAGESTREAMTAG

<span class="comment"># create image stream tag latest to follow another as alias</span>
oc tag --alias IMAGESTREAM:IMAGESTREAMTAG IMAGESTREAM:latest

<span class="comment"># create deployment from imagestream</span>
oc new-app --name test --image-stream=IMAGESTREAM:IMAGESTREAMTAG</code></pre>
  </div>

  <p>Pushing an image to the internal registry also updates the corresponding image stream. The image stream is created if it doesn't exist.</p>

  <p>Importing images from an external registry makes use of the <code>docker-registry</code> secrets in the project for authentication. However, mirroring the image in the internal registry avoids authentication to the external registry for the pull:er.</p>

</div>

<div>

  <p>Set an image stream's lookup policy to <code>local</code> to enable Kubernetes resources to reference image stream tags instead image URLs. That is, if a pod references <code>mysql-80:1-159</code> in its <code>image</code> field then the location in the image stream tag <code>mysql-80:1-159</code> (in the same project) is pulled without an upstream registry lookup.</p>

  <div class="code"><pre><code
>oc import-image registry.redhat.io/rhel8/mysql-80:1-159 --confirm
oc set image-lookup mysql-80:1-159                  <span class="comment"># set lookupPolicy.local to true</span>
oc run mysql --image=mysql-80:1-159 ...             <span class="comment"># pulls image stream tag's image</span>
oc set image-lookup --enabled=false mysql-80:1-159  <span class="comment"># set lookupPolicy.local to false</span>
oc set image-lookup  <span class="comment"># list lookup policies of image streams in project</span></code></pre>
  </div>

  <h2 id="1-3"><a href="#1-3"></a>Running image builds</h2>

  <p>The command <code>new-build</code> creates a <code>BuildConfig</code> and <code>ImageStream</code>. The build configuration starts a build pod that builds and pushes an image to an image stream or image registry.</p>
  
  <p>A <code>BuildConfig</code>'s build strategy is either to run a Dockerfile or a source-to-image (S2I) builder image. Input files are put in the context directory of the build and can be cloned from a Git repository, be uploaded from the command-line, or be sourced from another image. Cloning and uploading are mutually exclusive inputs.</p>

  <p>Below is an example of a Dockerfile build with uploaded files for context. The Dockerfile uses two images that are first imported into the project <code>common</code>. The <code>FROM</code> instructions use the internal repository.</p>

  <div class="code"><pre><code
>oc login -u linda && project env1  <span class="comment"># the build will run in env1</span>

<span class="comment"># create imagestreams in project common for imported images</span>
oc import-image gcc:11.2.0 --confirm -n common \
  --from=docker.io/library/gcc:11.2.0
oc import-image ubi8-minimal:8.5 --confirm -n common \
  --from=registry.access.redhat.com/ubi8-minimal:8.5

<span class="comment"># create BuildConfig for binary upload and expecting a Dockerfile</span>
oc new-build --name main --binary --strategy=docker
oc get buildconfig main o yaml  <span class="comment"># view BuildConfig</span>

<span class="comment"># create a Dockerfile (multi-staged)</span>
REGISTRY=$(oc registry info --internal)
cat &gt; Dockerfile &lt;&lt;&lt;"
FROM ${REGISTRY}/common/gcc:11.2.0 AS BUILD
WORKDIR /tmp/workdir/
COPY . .
RUN gcc -c -Wall -Werror -pthread -O2 -o main.o main.c && \
    gcc -o main main.o -pthread && \
    chmod 0770 main
FROM ${REGISTRY}/common/ubi8-minimal:8.5
COPY --from=BUILD --chown=0:0 /tmp/workdir/main /main
USER 1001:0
WORKDIR /
ENTRYPOINT [\"/main\"]"

<span class="comment"># start a build with files in current directory</span>
oc start-build main --from-dir=. --follow

<span class="comment"># view the pushed image from imagestream tag latest</span>
oc describe imagestreamtag main:latest

<span class="comment"># test run it</span>
oc run mainpod --image=$(oc registry info --public)/env1/main:latest --restart=Never
oc logs mainpod
oc delete pod mainpod</code></pre>
  </div>

  <h3 id="1-3-1"><a href="#1-3-1"></a>Build from Git source</h3>

  <p>Below, source files are cloned instead of uploaded into the build context. The same Dockerfile as above is in the repository. The build is started automatically.</p>

  <div class="code"><pre><code
><span class="comment"># create and start build from directory gcc</span>
oc new-build --name main --strategy=docker \
  common/ubi8-minimal:8.5~https://github.com/linjan2/example-repo.git --context-dir=gcc
oc logs buildconfig/main --follow</code></pre>
  </div>

</div>

<div>

  <p>With Git-builds, the syntax <code>IMAGESTREAM:TAG~URL#REF</code> is needed to stop <code>oc new-build</code> from first cloning the source outside the build in order to the parse the source files.</p>

  <p>For the Docker strategy the <code>BuildConfig</code>'s <code>dockerStrategy.from</code> field replaces the last <code>FROM</code> instruction in the Dockerfile. The field <code>dockerStrategy.from</code> can be removed to reference the registry's image directly. The node will pull the image when building.</p>

  <div class="code"><pre><code
>oc patch buildconfig main -p '{"spec":{"strategy":{"dockerStrategy":{"from": null}}}}'</code></pre>
  </div>

  <h4 id="1-3-1-1"><a href="#1-3-1-1"></a>Set build source secrets when cloning</h4>

  <p>Git repository credentials can be provided to builds using build source secrets. Create a secret for HTTP using an access token or for SSH using keys. Then either create the build with <code>--source-secret</code> or set the secret as below.</p>

  <div class="code"><pre><code
>oc set build-secret --source buildconfig/BUILDCONFIG SECRET</code></pre>
  </div>

  <p>A basic authentication secret for HTTP uses <code>username</code> and <code>password</code> data. Or, generate a GitHub personal access token for the <code>password</code> at <a href="https://github.com/settings/tokens/new">github.com/settings/tokens/new</a>. A token password can be used without a username.</p>

  <div class="code"><pre><code
><span class="comment"># sets both username and password</span>
oc create secret generic git-http-auth --type=kubernetes.io/basic-auth \
  --from-literal=username=USERNAME --from-literal=password=PASSWORD
<span class="comment"># or set password from token</span>
oc create secret generic git-http-auth --type=kubernetes.io/basic-auth \
  --from-literal=password=TOKEN

oc new-build --name main --strategy=docker \
  common/ubi8-minimal:8.5~https://github.com/linjan2/example-repo.git --context-dir=gcc \
  --source-secret=git-http-auth</code></pre>
</div>
  
  <p>Use <code>ssh-keygen</code> to generate the SSH keys; the public key should be uploaded to the repository. <code>ssh-keyscan</code> can get the repository's host key for <code>known_hosts</code>.</p>

  <div class="code"><pre><code
><span class="comment"># generate sshkey and sshkey.pub without passphrase</span>
ssh-keygen -C "crc-builder" -f sshkey -N ''

oc create secret generic git-ssh-auth --type=kubernetes.io/ssh-auth \
  --from-file=ssh-privatekey=sshkey \
  --from-file=ssh-publickey=sshkey.pub \
  --from-literal=known_hosts="$(ssh-keyscan -t rsa github.com)"

oc new-build --name main --strategy=docker \
  common/ubi8-minimal:8.5~git@github.com:linjan2/example-repo.git --context-dir=gcc \
  --source-secret=git-ssh-auth</code></pre>
  </div>

  <h3 id="1-3-2"><a href="#1-3-2"></a>Pushing the built image to a remote repository</h3>

  <p>Instead of pushing the resulting image to an image stream the build can push to a remote image repository. The output of a new build is set to a docker.io image below.</p>

  <div class="code"><pre><code
>oc new-build --name main --strategy=docker \
  common/ubi8-minimal:8.5~https://github.com/linjan2/example-repo.git --context-dir=gcc \
  --to-docker --to=docker.io/linjan2/main:123</code></pre>
  </div>

  <p>Credentials for the remote repository can be provided with a <code>docker-registry</code> secret added to the build configuration. The secret <code>docker-auth</code> is set below.</p>

  <div class="code"><pre><code
><span class="comment"># set push secret on new build</span>
oc new-build --name main --strategy=docker \
  common/ubi8-minimal:8.5~https://github.com/linjan2/example-repo.git --context-dir=gcc \
  --push-secret=docker-auth --to-docker --to=docker.io/linjan2/main:123

<span class="comment"># set push secret on existing build</span>
oc set build-secret --push bc/main docker-auth</code></pre>
  </div>

</div>

<div>

  <h3 id="1-3-3"><a href="#1-3-3"></a>Using S2I image builds</h3>

  <p>The source-to-image strategy (S2I) runs the specified "builder" image and starts a build script inside it along with the cloned source files. It then commits the container to an image.</p>

  <p>When the builder image is run it's provided the source files and then the script <code>assemble</code> is started inside it. The running container is then committed and that image's command is set to the script <code>run</code>.</p>

  <p>If the build is configured as incremental, then <code>save-artifacts</code> is started inside the container of the previous build's image before <code>assemble</code> is started in the current build. The tar archive created in <code>save-artifacts</code> is provided to the container of the current build.</p>

  <p>A builder image is created below using Buildah with a base image containing the <code>tar</code> utility. A working directory is created and S2I-specific labels are set to indicate where the S2I-scripts are and the destination for the source files (<code>src/</code>) and artifacts (<code>artifacts/</code>).</p>

  <div class="code"><pre><code
>ctr=$(registry.access.redhat.com/ubi8/ubi:latest)

buildah config \
  --label io.openshift.tags=builder \
  --label io.openshift.s2i.scripts-url=image:///usr/libexec/s2i \
  --label io.openshift.s2i.destination=/tmp \
  --label io.openshift.s2i.assemble-user=1001 \
  --cmd /usr/libexec/s2i/usage \
  --user 1001:0 \
  --entrypoint '' \
  --workingdir /1001 \
  ${ctr}
buildah run --user 0 ${ctr} -- chown 1001:0 /1001
buildah run --user 0 ${ctr} -- chmod 770 /1001
buildah copy --chown 1001:0 --chmod 770 ${ctr} \
  assemble save-artifacts run usage \
  /usr/libexec/s2i

buildah commit ${ctr} ubis2i:1</code></pre>
  </div>

  <p>The <code>assemble</code> script is shown below. If started with artifacts it moves them to the working directory. The source files are copied the working directory. Text files are created to simulate both the built application and the artifacts.</p>

  <div class="code"><pre><code
><span class="comment"># assemble</span>
#!/bin/bash -e

if [ -d /tmp/artifacts/ ]  <span class="comment"># if started with artifacts</span>
then
  echo "---> Restoring build artifacts..."
  shopt -s dotglob         <span class="comment"># for mv if artifacts have a leading dot</span>
  mv /tmp/artifacts/* ./
  shopt -u dotglob
fi

echo "---> Installing application source..."
cp -R /tmp/src/. ./

echo "---> Building application from source..."
[ ! -f date.txt ] && date > artifact.txt  <span class="comment"># simulate an artifact file</span>
date > application.txt                    <span class="comment"># simulate an application file</span></code></pre>
  </div>

  <p>The <code>save-artifacts</code> script is shown below. It creates a tar archive with the artifacts to stdout. Only <code>artifact.txt</code> is added as an artifact here.</p>

  <div class="code"><pre><code
><span class="comment"># save-artifacts</span>
tar -c -f - artifact.txt</code></pre>
  </div>

  <p>The <code>run</code> script is shown below. It starts the application with <code>exec</code>, which is just a terminating <code>cat</code> process in this case.</p>

  <div class="code"><pre><code
><span class="comment"># run</span>
exec cat application.txt artifact.txt</code></pre>
  </div>

</div>

<div>

  <p>The <code>usage</code> script set as the builder image's command is shown below. It outputs usage information.</p>
  
  <div class="code"><pre><code
><span class="comment"># usage</span>
echo 'Run as S2I builder'</code></pre>
  </div>

  <p>Push the image to a new image stream using the route to the cluster's internal registry.</p>

  <div class="code"><pre><code
>buildah login --username $(oc whoami) --password "$(oc whoami --show-token)" \
  $(oc registry info --public)

buildah push localhost/ubis2i:1 \
  docker://$(oc registry info --public)/env1/ubis2i:1

oc describe imagestreamtag ubis2i:1 -n env1</code></pre>
  </div>

  <p>Create a source-to-image build by using the strategy <code>source</code>. The initial non-incremental build is started automatically. When the initial build is finished, the build configuration's field <code>spec.strategy.sourceStrategy.incremental</code> is patched to <code>true</code>. The next build will then run <code>save-artifacts</code> using the previous build and provide the artifact files in the current build's directory <code>/tmp/artifacts</code>.</p>

  <div class="code"><pre><code
>oc new-build --name myapp --strategy=source \
  env1/ubis2i:1~git@github.com:linjan2/example-repo.git --source-secret=git-ssh-auth

oc logs -f bc/myapp  <span class="comment"># wait for the first build to push the initial image</span>

oc patch bc/myapp -p '{"spec":{"strategy":{"sourceStrategy":{"incremental": true}}}}'
oc start-build myapp --follow</code></pre>
  </div>

  <p>The scripts <code>assemble</code>, <code>run</code>, and <code>save-artifacts</code> can also be provided in the source directory <code>.s2i/bin</code> to override the ones in the builder image.</p>

  <p>Since the builder image will also be used as the runtime image, all the build dependencies are still present there.</p>

  <h4 id="1-3-3-1"><a href="#1-3-3-1"></a>Starting multistage builds with different images</h4>

  <p>A build can receive files from another image and this enables  multistage builds where the runtime image is not the same image as the builder. The first build creates artifacts inside its image, and then a second build copies them into its own image.</p>

  <p>Below, a source-to-image build is created to produce an image with a built application which a second source-to-image build copies. <code>--source-image-path</code> specifies which absolute path from the source image to copy into which relative context path of the current build.</p>

  <div class="code"><pre><code
><span class="comment"># create application with ubis2i:1</span>
oc new-build --name appbuild --strategy=source \
  env1/ubis2i:1~git@github.com:linjan2/example-repo.git --source-secret=git-ssh-auth

<span class="comment"># run application with ubis2i:2</span>
oc new-build --name apprun --strategy=source \
  --image-stream=env1/ubis2i:2 \
  --source-image=env1/appbuild:latest \
  --source-image-path=/1001/application.txt:./</code></pre>
  </div>

  <p>The <code>assemble</code> script for the first build's image <code>ubis2i:1</code> is below. It creates the application file in the working directory (i.e. as <code>/1001/application.txt</code>).</p>

  <div class="code"><pre><code
>echo "---> Building application from source..."
date > application.txt</code></pre>
  </div>

  <p>The <code>assemble</code> script for the second build's image <code>ubis2i:1</code> is below. It accesses the provided files in <code>/tmp/src</code>, since that is the build context for this source-to-image build.</p>

  <div class="code"><pre><code
>echo "---> Installing application..."
cp /tmp/src/application.txt .</code></pre>
  </div>

</div>

<div>

  <p>The second stage build can also clone repository source files along with the source image files. The destination directory for all source files is <code>/tmp/src</code>. Note that the value for <code>--context-dir</code> must match the prefix of the destination directory for <code>--source-image-path</code> as shown below. For the <code>assemble</code> script the source files are in <code>/tmp/src/mycontextdir</code>.</p>

  <div class="code"><pre><code
>oc new-build --name apprun --strategy=source \
  env1/ubis2i:1~git@github.com:linjan2/example-repo-2.git --source-secret=git-ssh-auth \
  --context-dir=mycontextdir \
  --source-image=env1/appbuild:latest \
  --source-image-path=/1001/application.txt:mycontextdir/</code></pre>
  </div>

  <p>A Dockerfile build can also receive source image files into its context. Below, an inline Dockerfile is used to build the image. <code>COPY</code> accesses the context files provided from the source image.</p>

  <div class="code"><pre><code
>oc new-build --name apprun \
  --source-image=env1/appbuild:latest \
  --source-image-path=/1001/application.txt:./ \
  --dockerfile '
FROM env1/ubi:2
COPY application.txt ./
CMD ["cat", "application.txt"]'</code></pre>
  </div>

  <h3 id="1-3-4"><a href="#1-3-4"></a>Triggering builds with webhooks</h3>

  <p>A build configuration can be triggered by HTTP POST requests to a URL path in the cluster domain. The payload format depends on the type of trigger. For generic triggers an optional YAML payload can contain the fields shown below. Other trigger types expect payloads sent by GitHub, GitLab, and BitBucket.</p>

  <div class="code"><pre><code
>git:
  uri: "GIT URL"
  ref: "REF"
  commit: "COMMIT HASH"
  author:
    name: "NAME"
    email: "EMAIL"
  committer:
    name: "NAME"
    email: "EMAIL"
  message: "COMMIT MESSAGE"
  env: 
  - name: "NAME"
    value: "VALUE"</code></pre>
  </div>

  <p>The <code>ref</code> parameter must match the value of <code>buildconfig.spec.source.git.ref</code> which is <code>master</code> if unset in the build configuration. The <code>commit</code> parameter can be any commit hash, regardless of the payload's <code>ref</code>, and this is what's cloned for the source strategy. The HTTP response is the new <code>Build</code> resource in JSON if a build was started; otherwise a <code>Status</code> resource JSON with <code>status</code> and <code>message</code> fields is sent.</p>

  <p>Webhook triggers can be changed with <code>oc set</code>. Setting environment variables with a generic webhook payload requires <code>buildconfig.spec.triggers.generic.allowEnv</code> to be <code>true</code>.</p>

  <div class="code"><pre><code
>oc set triggers bc/name --from-webhook --remove   <span class="comment"># remove all generic webhooks</span>
oc set triggers bc/name --from-webhook            <span class="comment"># add a generic webhook</span>
oc set triggers bc/name --from-webhook-allow-env  <span class="comment"># add a generic webhook with allowEnv</span>
oc set triggers bc/name --from-github             <span class="comment"># add a GitHub webhook</span>
</code></pre>
</div>

  <p>To use <code>curl</code> with the webhook endpoint, fetch and use the API server's certificate chain as trusted CA. OpenSSL is used below to connect to <code>api.crc.testing:6443</code> for this.</p>

  <div class="code"><pre><code
>host api.crc.testing  <span class="comment"># check lookup; CRC should've added cluster IP to /etc/hosts</span>
<span class="comment"># extract the certificate chain from the API server</span>
openssl s_client -showcerts -connect api.crc.testing:6443 &lt;/dev/null \
  | sed -n '/BEGIN CERTIFICATE/,/END CERTIFICATE/p' &gt; api.crc.testing.crt</code></pre>
</div>

  <p>Alternatively, as administrator, extract the entire root CA bundle from the configuration.</p>

</div>

<div>

  <div class="code"><pre><code
><span class="comment"># extract the root CA certificate chain for the API server (as the file "ca.crt")</span>
oc extract configmap/kube-root-ca.crt -n openshift-apiserver --keys=ca.crt
mv ca.crt kube-root-ca.crt  <span class="comment"># rename certificate chain file</span></code></pre>
  </div>
  
  <p>The webhook's secret identifier is extracted from the build configuration and then the webhook is triggered with a specific Git commit hash below.</p>

  <div class="code"><pre><code
>oc describe bc/name  <span class="comment"># view the webhooks' URL</span>

SECRETS=($(oc get bc/main -o jsonpath='{.spec.triggers[*].generic.secret}'))
SECRET=${SECRETS[0]}  <span class="comment"># get the first generic webhook's secret identifier (using shell array)</span>
NAMESPACE=env1
BC=name
CACERT=api.crc.testing.crt  <span class="comment"># or kube-root-ca.crt</span>

curl https://api-crc-testing:6443/oapi/v1/namespaces/${NAMESPACE}/buildconfigs/${BC}/webhooks/${SECRET}/generic \
  --cacert ${CACERT} -H 'Content-Type: application/yaml' --data \
'git:
  ref: master
  commit: "3036684"'</code></pre>
  </div>

  <h3 id="1-3-5"><a href="#1-3-5"></a>Adding a build post-commit hook</h3>

  <p>A build configuration's post-commit hook runs the built image with a modified entrypoint before it's pushed to the registry. If the return code is non-zero the build fails. The entrypoint is set to either a command, a script, or to the already set entrypoint with other arguments.</p>

  <div class="code"><pre><code
><span class="comment"># set post-commit hook with overwritten entrypoint</span>
oc set build-hook bc/name --post-commit --command -- env  <span class="comment"># doesn't unset postCommit.args</span>
oc set build-hook bc/name --post-commit --script='env && echo hello'
<span class="comment"># use entrypoint with other arguments</span>
oc patch bc/name -p '{"spec":{"postCommit":{"command": null, "args":["arg1","arg2"]}}}'
<span class="comment"># remove post-commit hook</span>
oc set build-hook bc/name --post-commit --remove</code></pre>
  </div>
    

  <h2 id="1-4"><a href="#1-4"></a>Using deployments</h2>

  <p>The <code>oc new-app</code> command works like <code>oc new-build</code> when provided a Git URL, but in addition to creating build resources it also creates a deployment. The deployment starts a pod from the image stream that the build configuration pushes its image to.</p>

  <div class="code"><pre><code
><span class="comment"># start a build and create deployment</span>
oc new-app --name name --strategy=source --source-secret=git-ssh-auth \
  env1/ubis2i:1~git@github.com:linjan2/example-repo.git#main

<span class="comment"># alternatively, create deployment from image stream</span>
oc new-app --name name env1/name:latest

oc delete all -l app=name  <span class="comment"># delete resources with the automatically assigned label</span></code></pre>
  </div>

  <p>A service resource is created by <code>new-app</code> if the deployed image has metadata for exposing a port. A service to the pod's port 8080 is created manually below. The service's name sets its pod selector with the label <code>app=name</code>. <code>oc new-app</code> labels all its created resources as such, which includes the deployed pods.</p>

  <div class="code"><pre><code
><span class="comment"># create service "name" to target port 8080 with selector app=name</span>
oc create service clusterip name --tcp=8080:8080
oc expose service name  <span class="comment"># add route to service under the cluster domain</span></code></pre>
  </div>

  <p>When the deployment replicas is scaled up to more than one pod the service will balance traffic between them. Scale the replicas to zero to stop all the deployment's pods.</p>

  <div class="code"><pre><code
>oc scale deployment/name --replicas=2
oc rollout status deployment/name</code></pre>
  </div>

</div>

<div>
  
  <p>The deployment's <code>spec.template.spec.containers</code> array specifies the images of the pod. The image from the build is automatically filled in and another image can be patched in. Below, an image reference is taken from the image stream tag.</p>
  
  <div class="code"><pre><code
  >oc patch deployment name -p \
  "spec:
  template:
    spec:
      containers:
      - name: container2
        image: $(oc get istag is2:latest -o jsonpath='{.image.dockerImageReference}')"</code></pre>
  </div>

  <p>Logs from all pod containers can be prefixed with pod name and container name.</p>

  <div class="code"><pre><code
>oc logs name --all-containers --prefix --timestamps</code></pre>
  </div>

  <p>There's a re-deployment trigger on the deployment that restarts its specified container with a new image tag reference when its corresponding image stream tag is updated. A separate image trigger can be added for the other container.</p>

  <div class="code"><pre><code
>oc set triggers deployment  <span class="comment"># show deployment triggers</span>
<span class="comment"># add re-deployment when second container's image stream tag is updated</span>
oc set triggers deployment name --from-image=env1/is2:latest --containers=container2

oc set triggers deployment/name --manual  <span class="comment"># suspend automatic trigger</span>
oc set triggers deployment/name --auto  <span class="comment"># resume automatic trigger</span></code></pre>
  </div>

  <h3 id="1-4-1"><a href="#1-4-1"></a>Specifying pod containers' resource requests and limits</h3>

  <p>A deployment can specify CPU and memory resources for its pod containers. A pod is guaranteed its resource requests, but may burst up to its resource limits. Pod resources affect their scheduling onto nodes.</p>

  <div class="code"><pre><code
>oc set resources deployment name --containers=name \
  --limits=cpu=200m,memory=128Mi \
  --requests=cpu=100m,memory=128Mi</code></pre>
  </div>

  <h3 id="1-4-2"><a href="#1-4-2"></a>Adding environment variables, <code>ConfigMaps</code>, and <code>Secrets</code> to pods</h3>

  <p><code>ConfigMap</code> and <code>Secret</code> resources can be created from files and literals with corresponding keys. A key's name must be of the format <code>[-._a-zA-Z0-9]+</code>.</p>

  <div class="code"><pre><code
>oc create configmap app-conf \
  --from-literal key1=value1 \
  --from-literal key2=value2
oc set data configmap/app-conf key3=value3  <span class="comment"># add an entry</span>
oc set data configmap/app-conf key3-        <span class="comment"># remove an entry</span>

oc create configmap trust-store --from-file=ca.crt=server-ca-bundle.crt

oc create secret generic credentials \
  --from-literal username=user \
  --from-literal password=$(openssl rand -base64 16)
oc set data secret/credentials username=loser  <span class="comment"># update an entry</span>

oc create secret generic client-cert \
  --from-file=client.crt \
  --from-file=client.key</code></pre>
  </div>

  <p>Add environment variables to the deployment's pod template with <code>oc set env</code>. When the key-value pairs from <code>ConfigMaps</code> and <code>Secrets</code> are used for environment variables the keys are converted to uppercase and dashes and dots become underscores.</p>

</div>

<div>

  <div class="code"><pre><code
>oc set env deployment/name ENV1=VAL1 ENV2=VAL2
oc set env -e - deployment/name &lt;&lt;&lt;$'key3=value3\nkey4=value4'  <span class="comment"># add from stdin (newline-delimited key-value pairs)</span>

<span class="comment"># add with name-prefix from ConfigMap to all containers</span>
oc set env --from=configmap/app-conf --prefix=APP_CONF_ deployment/name
<span class="comment"># add from Secret to the specified container</span>
oc set env --from=secret/credentials deployment/name --containers='container2'

<span class="comment"># remove a variable from a container</span>
oc set env deployment/name --containers=container2 APP_CONF_KEY2-
<span class="comment"># show the environment variables set</span>
oc set env --list deployment/name</code></pre>
</div>

  <p><code>oc set env</code> adds items to <code>deployment.spec.template.spec.containers.env</code>. This field provides the name of the environment variable. Another field is <code>envFrom</code> which only references <code>ConfigMaps</code> and <code>Secrets</code> and injects all key-value pairs using the key name as the environment variable. Keys with dots or dashes or a leading number would become invalid environment variables.</p>

  <p>Use <code>oc patch</code> to add an <code>envFrom</code> array to a container.</p>

  <div class="code"><pre><code
>oc patch deployment name -p \
'spec:
  template:
    spec:
      containers:
      - name: container2
        envFrom:
        - configMapRef:
            name: app-conf
        - secretRef:
            name: credentials'</code></pre>
  </div>

  <p>Volumes from <code>ConfigMaps</code> and <code>Secrets</code> can be added to the deployment and mounted into the pod containers' file systems.</p>

  <div class="code"><pre><code
><span class="comment"># mount one key-value entry as a file name conf.txt</span>
oc set volume deployment/name --containers=container2 --add --type=configmap \
  --name=app-conf-vol --configmap-name=app-conf \
  --mount-path /conf.txt --sub-path=key1

<span class="comment"># mount all key-value entries as files in directory /cred</span>
oc set volume deployment/name --containers=container2 --add --type=secret \
  --name=credentials-vol --secret-name=credentials --mount-path /cred
oc set volume deployment/name --remove --name=credentials-vol --containers=container2</code></pre>
  </div>

  <p>To mount a subset of files from a volume, patch in an array of keys and their respective path names on the volume.</p>

  <div class="code"><pre><code
><span class="comment"># provide only the key "password" as the file "pass" for mounting</span>
oc patch deployment/name -p \
'spec:
  template:
    spec:
      volumes:
      - name: credentials-vol
        secret:
          secretName: credentials
          items:
          - key: password
            path: pass'</code></pre>
  </div>

  <h4 id="1-4-2-1"><a href="#1-4-2-1"></a>Starting an init-container to copy <code>ConfigMap</code> data to a writable volume</h4>

  <p><code>ConfigMap</code> and <code>Secret</code> volumes are always mounted as read-only. To get around this, an <code>emptyDir</code> volume can be added with write permission and the data can be copied there. An init-container can copy files to a volume shared with a regular container.</p>

</div>

<div>

  <div class="code"><pre><code
><span class="comment"># create and emptyDir volume and mount in the regular container</span>
oc set volume deployment/name --containers='container2' --add \
  --type=emptyDir --name=shared-vol --mount-path /shared

<span class="comment"># create the conf-volume with owner-writable permissions (retained when copying)</span>
oc set volume deployment/name --add --type=configmap \
  --name=app-conf-vol --configmap-name=app-conf --default-mode=0640

<span class="comment"># add an init-container that copies files from the conf-volume to the shared volume</span>
oc patch deployment name -p \
'spec:
  template:
    spec:
      initContainers:
      - name: container2-init
        image: imagename:version
        command: ["sh", "-c", "cp -LR /conf/* /shared/"]
        volumeMounts:
          - mountPath: /conf
            name: app-conf-vol
          - mountPath: /shared
            name: shared-vol'</code></pre>
  </div>

  <h4 id="1-4-2-2"><a href="#1-4-2-2"></a>Using the downward API in resources</h4>

  <p>WIP</p>

  <h3 id="1-4-3"><a href="#1-4-3"></a>Adding persistent volume mounts</h3>

  <p>A <code>PersistentVolumeClaim</code> resource is created in the project below for persisted cluster storage. CRC provides static recycled persistent volumes on its single node which automatically bind to claims (without selectors or storage classes).</p>

  <div class="code"><pre><code
>cat &lt;&lt;&lt;'apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc1
spec:
  storageClassName: ""
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi' | oc create -f -</code></pre>
  </div>

  <p>Create a volume and mount with the type <code>pvc</code>.</p>

  <div class="code"><pre><code
><span class="comment"># create volume and mount from an existing persistent volume claim</span>
oc set volume deployment/name --add \
  --name=pvc-vol --type=pvc --mount-path=/data --claim-name=pvc1

<span class="comment"># create volume and mount with a new generated persistent volume claim</span>
oc set volume deployment/name --add --name=pvc-vol --type=pvc --mount-path=/data \
  --claim-name=pvc1 --claim-size=1Gi --claim-mode='ReadWriteOnce' --claim-class=''</code></pre>
  </div>

  <h3 id="1-4-4"><a href="#1-4-4"></a>Specifying pod containers' liveness probe and readiness probe</h3>
  
  <p>WIP</p>

  <h3 id="1-4-5"><a href="#1-4-5"></a>Using deployment rollout strategies</h3>

  <p>WIP</p>

  <h3 id="1-4-6"><a href="#1-4-6"></a>Creating services and secure routes</h3>

  <p>WIP</p>

</div>

<div>

  <h2 id="1-5"><a href="#1-5"></a>Managing persistent volumes</h2>

  <p>A <code>PersistentVolume</code> is a cluster resource used when mounting volumes in containers. The CRC cluster has prepared <code>hostMount</code>-type <code>PersistentVolumes</code> for mounting directories on the node into containers.</p>

  <div class="code"><pre><code
><span class="comment"># view cluster persistent volumes</span>
oc get persistentvolume
oc describe persistentvolume pv0002
<span class="comment"># view directories backing the hostMount persistent volumes</span>
oc debug node/$(oc get node -o jsonpath='{.items[0].metadata.name}') -- ls -lrt /host/mnt/pv-data</code></pre>
  </div>

  <p>A persistent volume is claimed with a <code>PersistentVolumeClaim</code> resource. The claims are then referenced in a <code>Pod</code> resource.</p>

  <p>A <code>hostPath</code> volume only works on the single-node cluster. Instead of manually creating persistent volumes, clusters can have storage operators for dynamic provisioning of persistent volumes backed by logical disk space. The provisioned volumes have a storage class name set which claims can specify to match up against. The storage class determines which provisioner to use.</p>

  <p>Even without a provisioner, the storage class can be set on a manually created persistent volume. A persistent volume claim can match with a volume using its <code>spec.storageClassName</code> and <code>spec.selector.matchLabels</code> fields. The claim can also specify the name of volume with <code>spec.volumeName</code>. A persistent volume backed by a directory on the node is created below with storage class and class.</p>

  <div class="code"><pre><code
>cat &lt;&lt;&lt;'apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv
  labels:
    volume: mypv
spec:
  storageClassName: mystorageclass
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 1Gi
  hostPath:
    path: /tmp/mypv
    type: DirectoryOrCreate
  persistentVolumeReclaimPolicy: Delete
  volumeMode: Filesystem' | oc create -f -</code></pre>
  </div>

  <p>A persistent volume is mounted on a node with a certain access mode. The access modes are:</p>

  <ul>
    <li><code>RWO</code>, <code>ReadWriteOnce</code>; the volume can be mounted on one node only in read-write mode.</li>
    <li><code>RWX</code>, <code>ReadWriteMany</code>; the volume can be mounted on many nodes in read-only mode.</li>
    <li><code>ROX</code>, <code>ReadOnlyMany</code>; the volume can be mounted on many nodes in read-write mode.</li>
  </ul>

  <p>The reclaim policy determines whether the backing storage is deleted (<code>Delete</code>) or retained (<code>Retain</code>) when its bound claim is deleted. The prepared CRC volumes have a policy of <code>Recycle</code> which retains and clears the same directories. If <code>Retain</code> is used, the persistent volume has to be recreated. <code>Delete</code> for <code>hostPath</code> in this case is only valid for directories under <code>/tmp/</code>.</p>

</div>

<div>

  <p>A persistent volume claim is created below. It matches against the previous persistent volume using both storage class and label selector. The size request must also be equal or less than the capacity of the matching volume.</p>

  <div class="code"><pre><code
>cat &lt;&lt;&lt;'apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
spec:
  storageClassName: mystorageclass
  selector:
    matchLabels:
      volume: mypv
  accessModes:
  - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi' | oc create -f -</code></pre>
  </div>

  <p>A volume is set on a deployment as shown below. A re-deployment is triggered.</p>

  <div class="code"><pre><code
>oc set volume deployment/mydeploy \
  --add --name=vol --mount-path=/data \
  --type=pvc --claim-name=mypvc</code></pre>
  </div>

  <h3 id="1-5-1"><a href="#1-5-1"></a>Using a default storage class</h3>

  <p>A storage class can be automatically set on persistent volume claims by adding the annotation <code>storageclass.kubernetes.io/is-default-class:"true"</code> on a created storage class. Otherwise the default storage class is empty which matches persistent volumes with no storage class.</p>
  
  <p>A default storage class without a provisioner is created below. The <code>storageClassName</code> field of a <code>PersistentVolumeClaim</code> will then default to <code>mystorageclass</code> when left unspecified.</p>

  <div class="code"><pre><code
>cat &lt;&lt;&lt;'apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mystorageclass
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: VolumeBindingImmediate
reclaimPolicy: Delete' | oc create -f -</code></pre>
  </div>

  <p>The storage class's <code>volumeBindingMode</code> can be <code>VolumeBindingImmediate</code> or <code>WaitForFirstConsumer</code> to delay the binding until a pod using the claim is scheduled.</p>

  <p>Installing a storage operator should also simultaneously create their storage class resource. <code>parameters</code> and <code>mountOptions</code> can also be set on a storage class.</p>

  <h3 id="1-5-2"><a href="#1-5-2"></a>Creating persistent volumes on a NFS backend</h3>

  <p>WIP</p>

</div>

<div>

  <h2 id="1-6"><a href="#1-6"></a>Using stateful sets</h2>

  <p>Stateful sets are used instead of deployments for providing consistent DNS host names (i.e. pod names) and persistent volumes to each replicated pod during re-scheduling. For example, databases with replicated storage can have one pod be identified as managing the master data storage and the rest as managing replicated copies. Database synchronization is handled by the database software itself, but it can be important that the pods to be spread out across nodes for resilience.</p>

  <p>A persistent volume claim is shared between replicas in a deployment. A stateful set has a volume claim template field instead of pod volumes. This generates separate claims for the pods and results in the replicas mounting different persistent volumes.</p>

  <p>If a pod is re-scheduled it will use the same persistent volume as previously. This requires the persistent volumes to be available from any node the pod is re-scheduled to. Scaling down a stateful set won't delete the associated persistent volumes. Pods of a stateful sets are scaled up and down in order of pod number.</p>

  <p>WIP</p>

  <h3 id="1-6-1"><a href="#1-6-1"></a>Setting node anti-affinity and pod taint</h3>

  <p>WIP</p>

</div>

<div>

  <h2 id="1-7"><a href="#1-7"></a>Using the Kubernetes API</h2>

</div>

</div><!--[ PAGES END ]-->

<div id="footer" class="remove-on-print"><span>2022, Linda Jansson.</span></div>

<!--[ SIDE PANEL ]-->
<input type="checkbox" id="side-panel-switch" aria-hidden="true">
<div id="side-panel" class="remove-on-print">
  <label id="side-panel-icon" for="side-panel-switch" aria-label="side panel toggle"></label>
  <div id="tabs" class="pack16">

    <input type="radio" id="tab-1" name="tab-control" aria-hidden="true" checked>
    <input type="radio" id="tab-2" name="tab-control" aria-hidden="true">
    <input type="radio" id="tab-3" name="tab-control" aria-hidden="true">
    <input type="radio" id="tab-4" name="tab-control" aria-hidden="true">

    <label for="tab-1" class="four" aria-label="side panel outline button">outline</label>
    <label for="tab-2" class="four" aria-label="side panel external button">external</label>
    <label for="tab-3" class="four" aria-label="side panel more button">more</label>
    <label for="tab-4" class="four" aria-label="side panel about button">about</label>

    <div class="sixteen" id="outline-tab">
      <a href="#title">OpenShift for application development</a>
      <ul>
        <li><a href="#1-1">Set up CodeReady containers</a>
          <ul>
            <li><a href="#1-1-1">Create users, projects, and service accounts</a></li>
            <li><a href="#1-1-2">Access internal image registry with Podman and Skopeo</a></li>
          </ul>
        </li>
        <li><a href="#1-2">Starting images</a>
          <ul>
            <li><a href="#1-2-1">Using OpenShift image streams</a></li>
          </ul>
        </li>
        <li><a href="#1-3">Running image builds</a>
          <ul>
            <li><a href="#1-3-1">Build from Git source</a></li>
            <li><a href="#1-3-2">Pushing the built image to a remote repository</a></li>
            <li><a href="#1-3-3">Using S2I image builds</a></li>
            <li><a href="#1-3-4">Triggering builds with webhooks</a></li>
            <li><a href="#1-3-5">Adding a build post-commit hook</a></li>
          </ul>
        </li>
        <li><a href="#1-4">Using deployments</a>
          <ul>
            <li><a href="#1-4-1">Specifying pod containers' resource requests and limits</li></a>
            <li><a href="#1-4-2">Adding environment variables, <code>ConfigMaps</code>, and <code>Secrets</code> to pods</li></a>
            <li><a href="#1-4-3">Adding persistent volume mounts</li></a>
            <li><a href="#1-4-4">Specifying pod containers' liveness probe and readiness probe</li></a>
            <li><a href="#1-4-5">Using deployment rollout strategies</li></a>
            <li><a href="#1-4-6">Creating services and secure routes</li></a>
          </ul>
        </li>
        <li><a href="#1-5">Managing persistent volumes</a>
          <ul>
            <li><a href="#1-5-1">Using a default storage class</a></li>
            <li><a href="#1-5-2">Creating persistent volumes on a NFS backend</a></li>
          </ul>
        </li>
        <li><a href="#1-6">Using stateful sets</a>
          <ul>
            <li><a href="#1-6-1">Setting node anti-affinity and pod taint</a></li>
          </ul>
        </li>
        <li><a href="#1-7">Using the Kubernetes API</a>
          <ul>
            <li>.</li>
          </ul>
        </li>
      </ul>
    </div>
    
    <div class="sixteen" id="reference-tab">
      <div class="reference pack16">
        <div class="two"></div><div class="fourteen em">Download site for CodeReady Containers.</div>
        <div class="four">Title:</div><div class="twelve">Create an OpenShift cluster | Red Hat OpenShift Cluster Manager</div>
        <div class="four">Author:</div><div class="twelve">Red Hat, Inc.</div>
        <div class="four">Version:</div><div class="twelve">2021-12-20</div>
        <div class="four">Link:</div><div class="twelve"><a href="https://console.redhat.com/openshift/create/local">console.redhat.com/openshift/create/local</a></div>
      </div>
      <div class="reference pack16">
        <div class="two"></div><div class="fourteen em">Red Hat CodeReady Containers Getting Started Guide, Release Notes, and Known Issues.</div>
        <div class="four">Title:</div><div class="twelve">Product Documentation for Red Hat CodeReady Containers</div>
        <div class="four">Author:</div><div class="twelve">Red Hat, Inc.</div>
        <div class="four">Version:</div><div class="twelve">Accessed 2021-12-20</div>
        <div class="four">Link:</div><div class="twelve"><a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_containers">access.redhat.com/documentation/en-us/red_hat_codeready_containers</a></div>
      </div>
      <div class="reference pack16">
        <div class="two"></div><div class="fourteen em">Red Hat OpenShift Container Platform documentation.</div>
        <div class="four">Title:</div><div class="twelve">Product Documentation for OpenShift Container Platform 4.9</div>
        <div class="four">Author:</div><div class="twelve">Red Hat, Inc.</div>
        <div class="four">Version:</div><div class="twelve">Accessed 2021-12-27</div>
        <div class="four">Link:</div><div class="twelve"><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9">access.redhat.com/documentation/en-us/openshift_container_platform/4.9</a></div>
      </div>
      <div class="reference pack16">
        <div class="two"></div><div class="fourteen em">OpenShift post-installation configuration.</div>
        <div class="four">Title:</div><div class="twelve">Post-installation configuration | OpenShift Container Platform 4.9</div>
        <div class="four">Author:</div><div class="twelve">Red Hat, Inc.</div>
        <div class="four">Version:</div><div class="twelve">Accessed 2022-01-15</div>
        <div class="four">Link:</div><div class="twelve"><a href="https://docs.openshift.com/container-platform/4.9/post_installation_configuration/index.html">docs.openshift.com/container-platform/4.9/post_installation_configuration/index.html</a></div>
      </div>
      <div class="reference pack16">
        <div class="two"></div><div class="fourteen em">Kubernetes documentation and guides.</div>
        <div class="four">Title:</div><div class="twelve">Kubernetes Documentation</div>
        <div class="four">Author:</div><div class="twelve">The Kubernetes Authors.</div>
        <div class="four">Version:</div><div class="twelve">Accessed 2022-02-07</div>
        <div class="four">Link:</div><div class="twelve"><a href="https://kubernetes.io/docs/home/">kubernetes.io/docs/home/</a></div>
      </div>
      <div class="reference pack16">
        <div class="two"></div><div class="fourteen em">OpenShift API documentation.</div>
        <div class="four">Title:</div><div class="twelve">API index</div>
        <div class="four">Author:</div><div class="twelve">Red Hat Inc.</div>
        <div class="four">Version:</div><div class="twelve">Accessed 2021-12-28</div>
        <div class="four">Link:</div><div class="twelve"><a href="https://docs.openshift.com/container-platform/4.9/rest_api/index.html">docs.openshift.com/container-platform/4.9/rest_api/index.html</a></div>
      </div>
      <div class="reference pack16">
        <div class="two"></div><div class="fourteen em">Kubernetes API documentation.</div>
        <div class="four">Title:</div><div class="twelve">Kubernetes API Reference Docs</div>
        <div class="four">Author:</div><div class="twelve">The Kubernetes Authors.</div>
        <div class="four">Version:</div><div class="twelve">Accessed 2021-12-23</div>
        <div class="four">Link:</div><div class="twelve"><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/">kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/</a></div>
      </div>
      <div class="reference pack16">
        <div class="two"></div><div class="fourteen em">Image registries' web search.</div>
        <div class="four">Link:</div><div class="twelve"><a href="https://catalog.redhat.com/software/containers/search">catalog.redhat.com/software/containers/search</a></div>
        <div class="four">Link:</div><div class="twelve"><a href="https://hub.docker.com/search?type=image">hub.docker.com/search?type=image</a></div>
        <div class="four">Link:</div><div class="twelve"><a href="https://registry.fedoraproject.org/">registry.fedoraproject.org/</a></div>
        <div class="four">Link:</div><div class="twelve"><a href="https://quay.io/search">quay.io/search</a></div>
      </div>
      
    </div>

    <div id="more-tab">
      <span role="img" aria-label="warning icon" class="icon">⚠</span>
    </div>

    <div id="about-tab">
      <p>This text was written for learning purposes. The website's text, HTML, and CSS were created by Linda Jansson.</p>
      <p>The goal of this text was to organize my notes into a comprehensive format so as to make it more available to others—and myself.</p>
      <p>Notify me of issues at <a href="https://github.com/linjan2/linjan2.github.io">github.com/linjan2/linjan2.github.io</a>.</p>
    </div>

  </div>
</div>

</body>
</html>
